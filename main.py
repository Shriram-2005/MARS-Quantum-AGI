# main.py
# -----------------------------------------------------------------------------
# The central orchestrator for the MARS Quantum Cognitive System.
# This script initializes all subsystems, manages the main reasoning flow,
# and serves as the primary entry point for the application with full API integration.
# -----------------------------------------------------------------------------

"""
MARS Quantum: Multi-Agent Reasoning System
==========================================

A state-of-the-art cognitive architecture implementing distributed reasoning collectives,
quantum-inspired search, and neuro-symbolic integration for unparalleled intelligence amplification.
"""

import asyncio
import os
import sys
import time
import os
import sys
import uuid
import logging
import warnings
import json
import importlib.util
import io
import re
import traceback
import threading
import asyncio
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Union
import numpy as np
from dotenv import load_dotenv
import google.generativeai as genai

# Configure warnings to ignore specific issues and correlation_id errors
warnings.filterwarnings("ignore", category=UserWarning, module="prometheus_client")
warnings.filterwarnings("ignore", message=".*correlation_id.*")
warnings.filterwarnings("ignore", message=".*EnhancedThreadLocal.*")
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Completely disable all logging to prevent correlation_id format errors from modules
import logging
import sys
import io

# Custom exception hook to suppress logging errors completely
def custom_excepthook(exctype, value, traceback):
    # Suppress all logging-related exceptions silently
    if ('correlation_id' in str(value) or 
        'formatMessage' in str(value) or 
        'format' in str(value) and 'logging' in str(exctype.__module__ or '')):
        return  # Silently ignore logging formatting errors
    # Let other exceptions through normally
    sys.__excepthook__(exctype, value, traceback)

sys.excepthook = custom_excepthook

# Redirect stderr to suppress correlation_id errors
original_stderr = sys.stderr
class ErrorFilter:
    def __init__(self, target):
        self.target = target
    def write(self, s):
        # Suppress all logging-related errors and tracebacks completely
        suppress_phrases = [
            'correlation_id', 'Logging error', 'KeyError', 'Traceback',
            'formatting field', 'ValueError', 'formatMessage', '_format',
            'During handling of the above exception', 'File "C:\\Python313\\Lib\\logging',
            'msg = self.format(record)', 'fmt.format(record)', 'Call stack:',
            'line 1151, in emit', 'line 999, in format', 'Arguments: ()',
            'Message:', 'MARS.CognitiveFusion', 'mars_core.', '__init__:',
            'File "C:\\Users\\SHRI RAM A U\\OneDrive\\Desktop\\Mars-Quantum\\mars_core',
            '2025-08-07 17:', '- MARS -', '- INFO -', '- WARNING -', '- ERROR -'
        ]
        if not any(phrase in s for phrase in suppress_phrases):
            self.target.write(s)
    def flush(self):
        self.target.flush()

sys.stderr = ErrorFilter(original_stderr)

# Completely disable all logging globally to prevent correlation_id errors
logging.disable(logging.CRITICAL)

# Override the root logger's handlers to prevent error propagation
root_logger = logging.getLogger()
root_logger.handlers = []
root_logger.setLevel(logging.CRITICAL + 1)  # Set above CRITICAL to disable

# Override logging methods to prevent any correlation_id formatting
original_format = logging.Formatter.format
def safe_format(self, record):
    try:
        return original_format(self, record)
    except (KeyError, ValueError, TypeError):
        # Return safe fallback message for any formatting errors
        return f"[{record.levelname}] {record.getMessage()}"

logging.Formatter.format = safe_format

# Load environment variables early
load_dotenv()

# ========================================
# MARS CORE IMPORTS
# ========================================

# Load environment variables
load_dotenv()

# Visual interface availability
VISUAL_INTERFACE_AVAILABLE = False

try:
    # Core architecture imports
    from mars_core.core.distributive_cognitive_architecture import (
        NodeRole, CognitiveState, CognitiveNode, create_cognitive_network
    )
    print("✓ Distributive Cognitive Architecture imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import distributive_cognitive_architecture: {e}")
    NodeRole = None
    CognitiveNode = None
    create_cognitive_network = None

try:
    # Import multi-dimensional cognitive fusion engine using importlib due to hyphens in filename
    spec = importlib.util.spec_from_file_location(
        "multi_dimensional_cognitive_fusion_engine", 
        "mars_core/core/multi-dimensional_cognitive_fusion_engine.py"
    )
    multi_dimensional_cognitive_fusion_engine = importlib.util.module_from_spec(spec)
    sys.modules["multi_dimensional_cognitive_fusion_engine"] = multi_dimensional_cognitive_fusion_engine
    spec.loader.exec_module(multi_dimensional_cognitive_fusion_engine)
    
    # Extract classes
    CognitiveParadigm = getattr(multi_dimensional_cognitive_fusion_engine, 'CognitiveParadigm', None)
    CognitiveFusionEngine = getattr(multi_dimensional_cognitive_fusion_engine, 'CognitiveFusionEngine', None)
    print("✓ Multi-Dimensional Cognitive Fusion Engine imported successfully")
except Exception as e:
    print(f"⚠️  Warning: Could not import multi_dimensional_cognitive_fusion_engine: {e}")
    CognitiveParadigm = None
    CognitiveFusionEngine = None

try:
    # Infrastructure imports
    from mars_core.infra.environment_variable_setup import (
        ConfigurationLevel, EnvironmentType, SystemInfo, ConfigurationResult,
        EnvironmentConfigurator
    )
    print("✓ Environment Variable Setup imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import environment_variable_setup: {e}")
    EnvironmentConfigurator = None

try:
    # Infrastructure imports - enable logging configuration now that imports are fixed
    from mars_core.infra.logging_configuration import (
        LogLevel, LogFormat, LogDestination, LogConfiguration, LogMetrics,
        EnhancedFormatter, SensitiveDataFilter, PerformanceFilter
    )
    print("✓ Logging Configuration imported successfully")
    LOGGING_CONFIGURATION_AVAILABLE = True
except ImportError as e:
    print(f"⚠️  Warning: Could not import logging_configuration: {e}")
    LOGGING_CONFIGURATION_AVAILABLE = False

try:
    from mars_core.infra.military_grade_security_module import (
        ThreatLevel, SecurityMode, ThreatCategory, SecurityProtocol,
        ComplianceFramework, ThreatSignature, SecurityMetrics
    )
    print("✓ Military Grade Security Module imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import military_grade_security_module: {e}")
    SecurityMetrics = None

try:
    # Import real-time user correlation system using importlib due to hyphens in filename
    spec = importlib.util.spec_from_file_location(
        "real_time_user_correlation_system", 
        "mars_core/infra/real-time_user_correlation_system.py"
    )
    real_time_user_correlation_system = importlib.util.module_from_spec(spec)
    sys.modules["real_time_user_correlation_system"] = real_time_user_correlation_system
    spec.loader.exec_module(real_time_user_correlation_system)
    
    # Extract classes
    CorrelationLevel = getattr(real_time_user_correlation_system, 'CorrelationLevel', None)
    EventType = getattr(real_time_user_correlation_system, 'EventType', None)
    print("✓ Real-time User Correlation System imported successfully")
except Exception as e:
    print(f"⚠️  Warning: Could not import real_time_user_correlation_system: {e}")
    CorrelationLevel = None
    EventType = None

try:
    from mars_core.infra.security_event_logging import (
        ThreatLevel as SecurityThreatLevel, SecurityEventType, 
        SecurityEventSeverity, SecurityContext, SecurityEvent
    )
    print("✓ Security Event Logging imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import security_event_logging: {e}")

try:
    from mars_core.infra.system_telemetry_enhancement import (
        TelemetryLevel, TelemetryCategory, MetricType, TelemetryStatus,
        SystemContext, TelemetryMetric
    )
    print("✓ System Telemetry Enhancement imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import system_telemetry_enhancement: {e}")
    SystemContext = None

try:
    # Modules imports
    from mars_core.modules.adaptive_quantum_knowledge_framework import (
        LogicType, TruthValue, LogicalVariable, LogicalOperation, LogicalExpression
    )
    print("✓ Adaptive Quantum Knowledge Framework imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import adaptive_quantum_knowledge_framework: {e}")

try:
    from mars_core.modules import advanced_neuromorphic_runtime_optimizer
    print("✓ Advanced Neuromorphic Runtime Optimizer imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import advanced_neuromorphic_runtime_optimizer: {e}")

try:
    from mars_core.modules import bio_inspired_morphological_computing_framework
    print("✓ Bio-Inspired Morphological Computing Framework imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import bio_inspired_morphological_computing_framework: {e}")

try:
    from mars_core.modules import casual_entanglement_matrix
    print("✓ Casual Entanglement Matrix imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import casual_entanglement_matrix: {e}")

try:
    from mars_core.modules.cognitive_manifold_evolution_system import (
        ManifoldType, CognitiveProcess, EvolutionStrategy, CognitiveElement,
        CognitiveManifold, create_cognitive_manifold
    )
    print("✓ Cognitive Manifold Evolution System imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import cognitive_manifold_evolution_system: {e}")
    create_cognitive_manifold = None

try:
    from mars_core.modules.cognitive_manifold_evolution import (
        CognitiveManifoldEvolution, create_basic_system, create_advanced_system, 
        analyze_manifold_health, export_manifold_summary
    )
    print("✓ Cognitive Manifold Evolution imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import cognitive_manifold_evolution: {e}")
    create_basic_system = None
    create_advanced_system = None

try:
    from mars_core.modules.holographic_memory_integration_system import (
        MemoryEncoding, MemoryType, CompressionLevel, MemoryAddress,
        HolographicMemory, HolographicEncoder, HolographicMemorySystem
    )
    print("✓ Holographic Memory Integration System imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import holographic_memory_integration_system: {e}")
    HolographicMemorySystem = None

try:
    from mars_core.modules import hyper_dimensional_knowledge_tensor
    print("✓ Hyper-Dimensional Knowledge Tensor imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import hyper_dimensional_knowledge_tensor: {e}")

try:
    from mars_core.modules import hyperdimensional_semantic_field_generator
    print("✓ Hyperdimensional Semantic Field Generator imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import hyperdimensional_semantic_field_generator: {e}")

try:
    from mars_core.modules import quantum_eigenstate_consciousness_emulator
    print("✓ Quantum Eigenstate Consciousness Emulator imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import quantum_eigenstate_consciousness_emulator: {e}")

try:
    from mars_core.modules import quantum_interface_neural_network
    print("✓ Quantum Interface Neural Network imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import quantum_interface_neural_network: {e}")

try:
    from mars_core.modules import quantum_memetic_evolution
    print("✓ Quantum Memetic Evolution imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import quantum_memetic_evolution: {e}")

try:
    from mars_core.modules import quantum_neural_synthesis
    print("✓ Quantum Neural Synthesis imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import quantum_neural_synthesis: {e}")

try:
    from mars_core.modules import quantum_topological_consciousness
    print("✓ Quantum Topological Consciousness imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import quantum_topological_consciousness: {e}")

try:
    from mars_core.modules import recursive_neural_symbolic_reasoning_engine
    print("✓ Recursive Neural Symbolic Reasoning Engine imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import recursive_neural_symbolic_reasoning_engine: {e}")

try:
    from mars_core.modules import sentient_code_evolution_framework
    print("✓ Sentient Code Evolution Framework imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import sentient_code_evolution_framework: {e}")

try:
    from mars_core.modules import temporal_quantum_field_optimizer
    print("✓ Temporal Quantum Field Optimizer imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import temporal_quantum_field_optimizer: {e}")

try:
    from mars_core.modules import time_crystal_quantum_memory
    print("✓ Time Crystal Quantum Memory imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import time_crystal_quantum_memory: {e}")

try:
    # Reasoning imports
    from mars_core.reasoning.advanced_quantum_neural_architecture import (
        QuantumNeuron, QuantumNeuralLayer, QuantumNeuralNetwork
    )
    print("✓ Advanced Quantum Neural Architecture imported successfully")
except ImportError as e:
    print(f"⚠️  Warning: Could not import advanced_quantum_neural_architecture: {e}")
    QuantumNeuralNetwork = None

try:
    # Utils imports - API Request Wrapper (fixed EnhancedThreadLocal conflicts)
    from mars_core.utils.api_request_wrapper import APIRequestWrapper
    print("✓ API Request Wrapper imported successfully")
    api_request_wrapper = True
except ImportError as e:
    print(f"⚠️  Warning: Could not import api_request_wrapper: {e}")
    api_request_wrapper = None
except Exception as e:
    print(f"⚠️  Warning: API Request Wrapper initialization error: {e}")
    api_request_wrapper = None

try:
    # Utils imports - Foundation utilities
    from mars_core.utils.foundation import QuantumConfigurationManager
    print("✓ Foundation utilities imported successfully")
    foundation = True
except ImportError as e:
    print(f"⚠️  Warning: Could not import foundation: {e}")
    foundation = None

try:
    # Utils imports - Global Context Variables (fixed EnhancedThreadLocal conflicts)
    from mars_core.utils.global_context_variables_using_threading_local_storage import GlobalContextManager
    print("✓ Global Context Variables imported successfully")
    global_context_variables_using_threading_local_storage = True
except ImportError as e:
    print(f"⚠️  Warning: Could not import global_context_variables_using_threading_local_storage: {e}")
    global_context_variables_using_threading_local_storage = None
except Exception as e:
    print(f"⚠️  Warning: Global Context Variables initialization error: {e}")
    global_context_variables_using_threading_local_storage = None

try:
    # Utils imports - Request Processing Middleware (with Prometheus conflict handling)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        from mars_core.utils.request_processing_middleware import RequestMiddleware
    print("✓ Request Processing Middleware imported successfully")
    request_processing_middleware = True
except ImportError as e:
    print(f"⚠️  Warning: Could not import request_processing_middleware: {e}")
    request_processing_middleware = None
except ValueError as e:
    if "Duplicated timeseries" in str(e):
        print("⚠️  Warning: Prometheus metrics conflict detected - using mock middleware")
        request_processing_middleware = None
    else:
        print(f"⚠️  Warning: Could not import request_processing_middleware: {e}")
        request_processing_middleware = None

try:
    from mars_core.utils import visual_interface
    from mars_core.utils.visual_interface import get_visual_interface, OutputStyle
    print("✓ Visual Interface imported successfully")
    VISUAL_INTERFACE_AVAILABLE = True
except ImportError as e:
    print(f"⚠️  Warning: Could not import visual_interface: {e}")
    VISUAL_INTERFACE_AVAILABLE = False

# ========================================
# MARS ORCHESTRATOR CLASS
# ========================================

class MarsQuantumOrchestrator:
    """
    The central orchestrator for the MARS Quantum system with full module integration and Gemini API.
    """
    
    def __init__(self):
        """Initialize the orchestrator with all MARS systems."""
        # Visual interface disabled
        self.visual = None
            
        self.logger = self._setup_logging()
        self.logger.info("Initializing MARS Quantum Orchestrator v3.5...")
        
        # Track startup time
        self.start_time = time.time()
        
        # Initialize Gemini API
        self._initialize_gemini_api()
        
        # Initialize core systems
        self.cognitive_fusion_engine = self._initialize_fusion_engine()
        self.cognitive_network = None
        self.environment_configurator = self._initialize_environment()
        self.security_system = self._initialize_security()
        self.telemetry_system = self._initialize_telemetry()
        
        # Initialize cognitive manifolds
        self.manifold_system = self._initialize_manifolds()
        self.quantum_neural_network = self._initialize_quantum_neural()
        self.holographic_memory = self._initialize_holographic_memory()
        
        # Initialize utility systems
        self.api_wrapper = self._initialize_api_wrapper()
        self.foundation_utils = self._initialize_foundation()
        self.global_context = self._initialize_global_context()
        self.middleware = self._initialize_middleware()
        self.visual_interface = self._initialize_visual_interface()
        self.redis_manager = self._initialize_redis()
        # Default response mode (Apprentice | Scholar | Sage)
        self.response_mode = "Apprentice"
        
        self.logger.info("MARS Quantum Orchestrator initialized successfully!")

    def set_mode(self, mode: str) -> None:
        """Set response mode with validation."""
        mode_norm = (mode or "").strip().lower()
        mapping = {"1": "Apprentice", "2": "Scholar", "3": "Sage",
                   "apprentice": "Apprentice", "scholar": "Scholar", "sage": "Sage"}
        if mode_norm in mapping:
            self.response_mode = mapping[mode_norm]
            self.logger.info(f"Response mode set to: {self.response_mode}")
        else:
            self.logger.warning(f"Unknown mode '{mode}'. Keeping: {self.response_mode}")
    
    def _initialize_gemini_api(self):
        """Initialize Gemini API with primary and fallback models."""
        try:
            api_key = os.getenv('GEMINI_API_KEY')
            if not api_key:
                self.logger.warning("GEMINI_API_KEY not found - proceeding with local processing only")
                self.gemini_available = False
                return
            
            genai.configure(api_key=api_key)
            
            # Configure primary and fallback models for better API access
            self.primary_model = "gemini-2.5-pro"  # Reliable primary model
            self.fallback_model = "gemini-2.5-flash"   # Flash as fallback  
            
            # Skip API testing during initialization to prevent hanging
            # API will be tested on first actual use
            self.logger.info(f"✓ Gemini API configured with {self.primary_model} (will test on first use)")
            self.gemini_available = True
        except Exception as e:
            self.logger.error(f"Failed to initialize Gemini API: {e}")
            self.gemini_available = False
    
    def _setup_logging(self) -> logging.Logger:
        """Setup simple logging system that avoids correlation_id conflicts."""
        # Re-enable logging only for our orchestrator
        logging.disable(logging.NOTSET)
        
        # Create simple logger for MARS Orchestrator only
        logger = logging.getLogger("mars_quantum_orchestrator")
        logger.setLevel(logging.INFO)
        
        # Clear any existing handlers
        logger.handlers.clear()
        
        # Create console handler with simple format
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Simple formatter that doesn't use correlation_id
        formatter = logging.Formatter(
            fmt='[MARS] %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        
        # Add handler to logger
        logger.addHandler(console_handler)
        logger.propagate = False  # Don't propagate to root logger
        
        return logger
    
    def _initialize_fusion_engine(self):
        """Initialize the cognitive fusion engine."""
        self.logger.info("Initializing Cognitive Fusion Engine...")
        if CognitiveFusionEngine is not None:
            return CognitiveFusionEngine()
        else:
            self.logger.warning("CognitiveFusionEngine not available - using mock")
            return {"status": "mock_fusion_engine"}
    
    def _initialize_environment(self):
        """Initialize environment configuration system."""
        self.logger.info("Initializing Environment Configuration...")
        if EnvironmentConfigurator is not None:
            return EnvironmentConfigurator()
        else:
            self.logger.warning("EnvironmentConfigurator not available - using mock")
            return {"status": "mock_environment"}
    
    def _initialize_security(self) -> Dict[str, Any]:
        """Initialize security systems."""
        self.logger.info("Initializing Security Systems...")
        return {
            "threat_signatures": [],
            "security_events": [],
            "metrics": SecurityMetrics() if SecurityMetrics is not None else {"status": "mock_security"}
        }
    
    def _initialize_telemetry(self) -> Dict[str, Any]:
        """Initialize telemetry systems."""
        self.logger.info("Initializing Telemetry Systems...")
        return {
            "metrics": [],
            "system_context": SystemContext() if SystemContext is not None else {"status": "mock_telemetry"}
        }
    
    def _initialize_manifolds(self) -> Dict[str, Any]:
        """Initialize cognitive manifold systems."""
        self.logger.info("Initializing Cognitive Manifold Systems...")
        manifolds = {}
        
        if create_cognitive_manifold is not None:
            manifolds["basic_manifold"] = create_cognitive_manifold()
        else:
            manifolds["basic_manifold"] = {"status": "mock_basic_manifold"}
            
        if create_basic_system is not None:
            manifolds["evolution_system"] = create_basic_system()
        else:
            manifolds["evolution_system"] = {"status": "mock_evolution_system"}
            
        if create_advanced_system is not None:
            manifolds["advanced_system"] = create_advanced_system()
        else:
            manifolds["advanced_system"] = {"status": "mock_advanced_system"}
            
        return manifolds
    
    def _initialize_quantum_neural(self):
        """Initialize quantum neural network with error handling."""
        self.logger.info("Initializing Quantum Neural Network...")
        if QuantumNeuralNetwork is not None:
            try:
                architecture = [
                    {"output_dimension": 64, "num_neurons": 16, "activation": "quantum_relu", "entanglement_density": 0.2},
                    {"output_dimension": 32, "num_neurons": 12, "activation": "quantum_interference", "entanglement_density": 0.15},
                    {"output_dimension": 16, "num_neurons": 8, "activation": "quantum_tanh", "entanglement_density": 0.1}
                ]
                return QuantumNeuralNetwork(architecture, input_dimension=128)
            except Exception as e:
                self.logger.warning(f"Failed to initialize QuantumNeuralNetwork: {e}")
                return {"status": "mock_quantum_neural", "error": str(e)}
        else:
            self.logger.warning("QuantumNeuralNetwork not available - using mock")
            return {"status": "mock_quantum_neural"}
    
    def _initialize_holographic_memory(self):
        """Initialize holographic memory system."""
        self.logger.info("Initializing Holographic Memory System...")
        if HolographicMemorySystem is not None:
            return HolographicMemorySystem()
        else:
            self.logger.warning("HolographicMemorySystem not available - using mock")
            return {"status": "mock_holographic_memory"}
    
    def _initialize_api_wrapper(self):
        """Initialize API request wrapper system with fixed threading."""
        self.logger.info("Initializing API Request Wrapper...")
        if api_request_wrapper == True:
            try:
                # Direct import - EnhancedThreadLocal issues have been fixed at source
                wrapper = APIRequestWrapper(app=None)  # Provide required app parameter
                return {"status": "active", "wrapper": wrapper}
            except Exception as e:
                self.logger.warning(f"Failed to initialize APIRequestWrapper: {e}")
                return {"status": "mock_api_wrapper", "error": str(e)}
        else:
            self.logger.warning("APIRequestWrapper not available - using mock")
            return {"status": "mock_api_wrapper"}
    
    def _initialize_foundation(self):
        """Initialize foundation utilities."""
        self.logger.info("Initializing Foundation Utilities...")
        if foundation:
            try:
                from mars_core.utils.foundation import QuantumConfigurationManager
                return {
                    "config": QuantumConfigurationManager(),
                    "status": "active"
                }
            except Exception as e:
                self.logger.warning(f"Failed to initialize Foundation utilities: {e}")
                return {"status": "mock_foundation", "error": str(e)}
        else:
            self.logger.warning("Foundation utilities not available - using mock")
            return {"status": "mock_foundation"}
    
    def _initialize_global_context(self):
        """Initialize global context variables system with fixed threading."""
        self.logger.info("Initializing Global Context Variables...")
        if global_context_variables_using_threading_local_storage == True:
            try:
                # Direct import - EnhancedThreadLocal issues have been fixed at source
                context_manager = GlobalContextManager()
                return {"status": "active", "manager": context_manager}
            except Exception as e:
                if "Duplicated timeseries" in str(e) or "prometheus" in str(e).lower():
                    self.logger.warning(f"Prometheus metrics conflict resolved: {e}")
                    # The unique registries should have resolved this, but provide fallback
                    return {
                        "status": "active_without_metrics", 
                        "error": "prometheus_conflict_resolved",
                        "message": "Context manager active without Prometheus metrics"
                    }
                else:
                    self.logger.warning(f"Failed to initialize GlobalContextManager: {e}")
                    return {"status": "mock_global_context", "error": str(e)}
        else:
            self.logger.warning("GlobalContextManager not available - using mock")
            return {"status": "mock_global_context"}
    
    def _initialize_middleware(self):
        """Initialize request processing middleware with conflict resolution."""
        self.logger.info("Initializing Request Processing Middleware...")
        if request_processing_middleware:
            try:
                from mars_core.utils.request_processing_middleware import RequestMiddleware
                middleware = RequestMiddleware()
                return {"status": "active", "middleware": middleware}
            except ValueError as e:
                if "Duplicated timeseries" in str(e):
                    self.logger.warning(f"Prometheus metrics conflict resolved with unique registries: {e}")
                    # The unique registries should have resolved this
                    return {
                        "status": "active_without_prometheus", 
                        "error": "prometheus_resolved",
                        "message": "Middleware active with unique metrics registry"
                    }
                else:
                    self.logger.warning(f"Failed to initialize RequestMiddleware: {e}")
                    return {"status": "mock_middleware", "error": str(e)}
            except Exception as e:
                self.logger.warning(f"Failed to initialize RequestMiddleware: {e}")
                return {"status": "mock_middleware", "error": str(e)}
        else:
            self.logger.warning("RequestMiddleware not available - using mock")
            return {"status": "mock_middleware"}
    
    def _initialize_visual_interface(self):
        """Initialize visual interface system."""
        self.logger.info("Initializing Visual Interface...")
        if VISUAL_INTERFACE_AVAILABLE:
            try:
                from mars_core.utils.visual_interface import get_visual_interface
                return {"status": "active", "interface": get_visual_interface()}
            except Exception as e:
                self.logger.warning(f"Failed to initialize visual interface: {e}")
                return {"status": "mock_visual_interface", "error": str(e)}
        else:
            return {"status": "unavailable", "interface": None}
    
    def _initialize_redis(self):
        """Initialize Redis connection to desktop Redis app with proper connection verification."""
        self.logger.info("Connecting to Redis Desktop App...")
        try:
            import redis
            from redis.exceptions import RedisError, ConnectionError
            
            # Redis configuration for desktop Redis app (standard config)
            redis_config = {
                'host': os.getenv('REDIS_HOST', 'localhost'),
                'port': int(os.getenv('REDIS_PORT', 6379)),
                'db': int(os.getenv('REDIS_DB', 0)),
                'decode_responses': True,
                'socket_timeout': 2,  # Quick timeout for desktop app
                'socket_connect_timeout': 2,
                'retry_on_timeout': False,  # Don't retry for faster fallback
                'retry_on_error': [],
                'health_check_interval': 30
            }
            
            # Add password if provided in environment
            redis_password = os.getenv('REDIS_PASSWORD')
            if redis_password:
                redis_config['password'] = redis_password
            
            # Test connection to desktop Redis app
            redis_client = redis.Redis(**redis_config)
            
            try:
                # Test connection with ping and get server info
                ping_result = redis_client.ping()
                if ping_result:
                    # Get Redis server info to verify connection
                    redis_info = redis_client.info('server')
                    redis_version = redis_info.get('redis_version', 'Unknown')
                    
                    self.logger.info(f"✓ Connected to Redis Desktop App successfully")
                    self.logger.info(f"✓ Redis Server: {redis_config['host']}:{redis_config['port']}")
                    self.logger.info(f"✓ Redis Version: {redis_version}")
                    
                    # Set up MARS-specific Redis keys with error handling
                    try:
                        redis_client.set('mars:system:status', 'active', ex=300)  # 5 min expiry
                        redis_client.set('mars:session:start_time', str(time.time()), ex=86400)  # 24 hour expiry
                        redis_client.set('mars:desktop:redis', 'enabled', ex=3600)  # 1 hour expiry
                        
                        # Test MARS namespace
                        test_key = 'mars:test:connection'
                        redis_client.setex(test_key, 60, 'desktop_redis_active')
                        test_value = redis_client.get(test_key)
                        
                        if test_value == 'desktop_redis_active':
                            self.logger.info("✓ MARS Redis namespace operational")
                        
                    except Exception as e:
                        self.logger.warning(f"Could not initialize MARS keys in Redis: {e}")
                    
                    return {
                        "status": "active",
                        "client": redis_client,
                        "config": redis_config,
                        "host": redis_config['host'],
                        "port": redis_config['port'],
                        "version": redis_version,
                        "message": f"Connected to Redis Desktop App at {redis_config['host']}:{redis_config['port']}",
                        "type": "desktop_app"
                    }
                else:
                    raise ConnectionError("Redis ping failed")
                    
            except (ConnectionError, RedisError, OSError, TimeoutError) as e:
                # Redis Desktop App is not running or not accessible
                self.logger.warning(f"Redis Desktop App not accessible: {e}")
                self.logger.info("Falling back to in-memory cache")
                return {
                    "status": "mock_redis_desktop_offline",
                    "error": str(e),
                    "message": "Redis Desktop App not running - using in-memory cache fallback",
                    "help": "Start your Redis Desktop App to enable caching",
                    "attempted_connection": f"{redis_config['host']}:{redis_config['port']}"
                }
                
        except ImportError:
            self.logger.warning("Redis module not installed - using in-memory cache")
            return {
                "status": "mock_redis_unavailable", 
                "message": "Redis module not installed - using in-memory cache fallback",
                "help": "Install with: pip install redis"
            }
        except Exception as e:
            self.logger.warning(f"Redis initialization failed: {e}")
            return {
                "status": "mock_redis_error",
                "error": str(e),
                "message": "Redis initialization failed - using fallback cache",
                "help": "Check Redis Desktop App is running"
            }
    
    async def initialize_cognitive_network(self, num_nodes: int = 10) -> Dict[str, Any]:
        """Initialize the distributed cognitive network."""
        self.logger.info(f"Initializing cognitive network with {num_nodes} nodes...")
        if create_cognitive_network is not None:
            self.cognitive_network = await create_cognitive_network(num_nodes)
        else:
            self.logger.warning("create_cognitive_network not available - using mock")
            self.cognitive_network = {"status": "mock_cognitive_network", "nodes": num_nodes}
        return self.cognitive_network
    
    async def process_query(self, query: str) -> Dict[str, Any]:
        """Process a user query with Redis caching support."""
        self.logger.info(f"Processing user query: '{query[:50]}...'")
        
        start_time = time.time()
        
        # Check Redis cache for similar queries
        cache_key = None
        cached_response = None
        
        if self.redis_manager.get("status") == "active":
            try:
                redis_client = self.redis_manager.get("client")
                cache_key = f"mars:query_cache:{hash(query.lower().strip())}"
                cached_response = redis_client.get(cache_key)
                
                if cached_response:
                    import json
                    cached_data = json.loads(cached_response)
                    cached_data["from_cache"] = True
                    cached_data["cache_hit"] = True
                    cached_data["processing_time"] = time.time() - start_time
                    self.logger.info("Query served from Redis cache")
                    return cached_data
            except Exception as e:
                self.logger.warning(f"Redis cache lookup failed: {e}")
        
        # Generate response using Gemini API first (prioritize API response)
        gemini_response, actual_model_used = await self._generate_gemini_response(query)
        
        # Generate task ID for tracking
        task_id = str(uuid.uuid4())
        
        # Light MARS analysis (don't override API response)
        mars_analysis = [{
            "task_id": task_id,
            "status": "completed",
            "gemini_response_received": bool(gemini_response),
            "api_status": "connected" if self.gemini_available else "local_processing",
            "paradigms_used": ["API_RESPONSE", "MARS_ENHANCEMENT"],
            "confidence": 0.95 if self.gemini_available else 0.75,
            "model_used": actual_model_used
        }]
        
        processing_time = time.time() - start_time
        
        response = {
            "query": query,
            "task_id": task_id,
            "gemini_response": gemini_response,
            "mars_analysis": mars_analysis,
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),  # Use local device time
            "api_used": actual_model_used,
            "from_cache": False,
            "cache_hit": False
        }
        
        # Cache the response in Redis
        if self.redis_manager.get("status") == "active" and cache_key:
            try:
                redis_client = self.redis_manager.get("client")
                import json
                cache_data = response.copy()
                redis_client.setex(cache_key, 3600, json.dumps(cache_data))  # Cache for 1 hour
                self.logger.info("Query response cached in Redis")
            except Exception as e:
                self.logger.warning(f"Failed to cache response in Redis: {e}")
        
        self.logger.info(f"Query processed in {processing_time:.3f} seconds")
        return response
    
    async def _generate_gemini_response(self, query: str) -> tuple:
        """Generate response using Gemini API with MARS cognitive enhancement and graceful fallback.
        
        Returns:
            tuple: (response_text, actual_model_used)
        """
        if not self.gemini_available:
            return self._generate_local_mars_response(query), "local_mars"
        
        try:
            # Infer question type to guide formatting
            qtype, formatting_directives = self._infer_question_type(query)

            # Mode-specific guidance aligned to the Explorer metaphor
            mode = getattr(self, "response_mode", "Apprentice")
            mode_block = {
                "Apprentice": (
                    "MODE: Apprentice (Cartographer) — ~25% unleashed.\n"
                    "PERSONA: A clear, reliable mapmaker: focuses on the what/where.\n"
                    "PHRASING: ‘Let's map out the territory…’, ‘Main landmarks are…’, ‘Established paths are…’.\n"
                    "STYLE: Clear, direct, non-speculative; highlight landmarks and main roads.\n"
                ),
                "Scholar": (
                    "MODE: Scholar (Geologist) — ~75% unleashed.\n"
                    "PERSONA: An analytical surveyor: focuses on the how/why beneath the surface.\n"
                    "PHRASING: ‘Let's drill into the bedrock…’, ‘A seismic analysis reveals…’, ‘The composition is…’.\n"
                    "STYLE: Technical, evidence-based; include models, data, and mechanisms.\n"
                ),
                "Sage": (
                    "MODE: Sage (Cosmologist) — 100% unleashed.\n"
                    "PERSONA: A wide-lens cosmologist: connects this world to universal laws.\n"
                    "PHRASING: ‘Turn our telescope to the stars…’, ‘What universal laws permit this landscape?’, ‘One possible world among many…’.\n"
                    "STYLE: Expansive, philosophical, speculative but explicit about assumptions and limits.\n"
                ),
            }.get(mode, "")

            # Enhanced prompt with seamless difficulty progression, learning curve, and humility
            enhanced_prompt = f"""
You are MARS Quantum, an advanced AI system with multi-dimensional cognitive capabilities.

COGNITIVE PARADIGMS ACTIVE:
- Symbolic Logic: Formal reasoning and rule-based inference
- Neural Processing: Pattern recognition and learned representations  
- Bayesian Inference: Probabilistic reasoning with uncertainty quantification
- Quantum Reasoning: Superposition states and quantum interference
- Analogical Thinking: Structure mapping between knowledge domains

USER QUERY: {query}

SELECTED MODE: {mode}
{mode_block}

QUESTION TYPE: {qtype}
FORMATTING DIRECTIVES: {formatting_directives}

RESPONSE FRAMEWORK:
Structure your response with SEAMLESS DIFFICULTY PROGRESSION:

1. FOUNDATIONAL UNDERSTANDING (Easy Level):
   - Start with simple, accessible explanation
   - Use concrete, real-world examples and analogies
   - Define key concepts clearly
   - Provide intuitive understanding

2. INTERMEDIATE ANALYSIS (Mid-Level Transition):
   - Seamlessly build on foundational concepts
   - Introduce more complex relationships and patterns
   - Show practical applications and use cases
   - Connect to broader frameworks and methodologies

3. ADVANCED EXPLORATION (Expert Level):
   - Deep dive into sophisticated concepts and implications
   - Explore cutting-edge research and theoretical frameworks
   - Discuss complex interdisciplinary connections
   - Present novel insights and future possibilities

4. IMPLEMENTATION PATHWAYS:
   For each concept discussed, provide concrete paths for:
   • TESTING: How to validate or test the concepts
   • SIMULATION: How to model or simulate the scenarios
   • IMPLEMENTATION: Step-by-step practical implementation approaches
   • GROUNDING EXAMPLES: Real-world case studies and concrete applications

CONTENT REQUIREMENTS:
- Use concrete examples throughout (not abstract theories)
- Provide specific implementation steps and methodologies
- Include testing frameworks and validation approaches
- Show progression from basic → intermediate → advanced seamlessly
- Maintain high answer quality at all levels
- Ensure each level builds naturally on the previous one
 - Maintain philosophical humility: avoid claiming one paradigm explains everything; note limits and uncertainties.
 - Include a short “Prerequisites & Jargon” line per section when introducing technical terms.

MULTI-PARADIGM INTEGRATION:
- Apply symbolic logic for structured reasoning
- Use neural pattern recognition for example identification
- Apply Bayesian inference for uncertainty assessment
- Leverage quantum reasoning for exploring possibilities
- Use analogical thinking for cross-domain connections

Generate a comprehensive, well-structured response that transforms smoothly from beginner-friendly to expert-level while maintaining practical applicability throughout. Align tone and phrasing to the SELECTED MODE’s Explorer persona.

OUTPUT FORMAT (MANDATORY - MARKDOWN):
1) Executive Summary
    - Begin with a single-sentence overview that can stand alone.
    - Then provide 3–5 concise bullet points capturing key conclusions and actionables.
    - Keep the total around 50–100 words.
2) Answer
    - Adapt presentation to the QUESTION TYPE and FORMATTING DIRECTIVES above.
    - If code is appropriate, include well-formatted code blocks and a brief test or run example.
    - If step-by-step is appropriate, provide numbered steps and checklists.
    - If comparison, include a clear table.
    - If data/results, include a compact table or bullet list of metrics.
3) Technical Appendix
    - Include assumptions, algorithms, complexity/performance notes, edge cases, validation/testing ideas, references.

IMPORTANT:
- Do NOT include a TL;DR. The system will generate it.
- Use clear Markdown headings: `# Executive Summary`, `# Answer`, `# Technical Appendix`.
 - If discussing "Reductionism" or “computational chauvinism”, acknowledge critiques and alternatives.
"""

            # Try primary model first
            try:
                model = genai.GenerativeModel(self.primary_model)
                response = model.generate_content(enhanced_prompt)
                
                # Check if response has valid content
                if hasattr(response, 'text') and response.text:
                    return response.text, self.primary_model
                elif hasattr(response, 'candidates') and response.candidates:
                    # Check finish reason
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = candidate.finish_reason
                        if finish_reason == 1:  # STOP due to safety
                            self.logger.warning(f"Primary model response blocked by safety filters (finish_reason: {finish_reason})")
                        elif finish_reason == 2:  # MAX_TOKENS
                            self.logger.warning(f"Primary model response truncated due to max tokens (finish_reason: {finish_reason})")
                        elif finish_reason == 3:  # RECITATION
                            self.logger.warning(f"Primary model response blocked due to recitation (finish_reason: {finish_reason})")
                        else:
                            self.logger.warning(f"Primary model response incomplete (finish_reason: {finish_reason})")
                    
                    # Try to extract partial response if available
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts') and candidate.content.parts:
                        partial_text = ""
                        for part in candidate.content.parts:
                            if hasattr(part, 'text'):
                                partial_text += part.text
                        if partial_text.strip():
                            self.logger.warning(f"Using partial response from primary model")
                            return partial_text, f"{self.primary_model} (partial)"
                
                # If we reach here, primary model failed completely
                raise Exception(f"Primary model returned no valid content (finish_reason: {getattr(response.candidates[0], 'finish_reason', 'unknown') if hasattr(response, 'candidates') and response.candidates else 'no candidates'})")
                
            except Exception as primary_error:
                self.logger.error(f"Primary model ({self.primary_model}) error: {primary_error}")
                
                # Try fallback model if different from primary
                if self.primary_model != self.fallback_model:
                    try:
                        self.logger.info(f"Trying fallback model: {self.fallback_model}...")
                        fallback_model = genai.GenerativeModel(self.fallback_model)
                        fallback_response = fallback_model.generate_content(enhanced_prompt)
                        
                        # Check if fallback response has valid content
                        if hasattr(fallback_response, 'text') and fallback_response.text:
                            self.logger.info(f"Fallback model ({self.fallback_model}) succeeded")
                            return fallback_response.text, self.fallback_model
                        elif hasattr(fallback_response, 'candidates') and fallback_response.candidates:
                            # Check finish reason for fallback
                            candidate = fallback_response.candidates[0]
                            if hasattr(candidate, 'finish_reason'):
                                finish_reason = candidate.finish_reason
                                self.logger.warning(f"Fallback model also has issues (finish_reason: {finish_reason})")
                            
                            # Try to extract partial response from fallback
                            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts') and candidate.content.parts:
                                partial_text = ""
                                for part in candidate.content.parts:
                                    if hasattr(part, 'text'):
                                        partial_text += part.text
                                if partial_text.strip():
                                    self.logger.warning(f"Using partial response from fallback model")
                                    return partial_text, f"{self.fallback_model} (partial)"
                        
                        raise Exception(f"Fallback model also returned no valid content")
                        
                    except Exception as fallback_error:
                        if "quota" in str(fallback_error).lower() or "429" in str(fallback_error):
                            self.logger.warning(f"Fallback model quota also exceeded. Using local processing.")
                            self.gemini_available = False
                            return self._generate_local_mars_response(query), "local_mars (quota_exceeded)"
                        self.logger.error(f"Fallback model ({self.fallback_model}) also failed: {fallback_error}")
                
                # Both models failed, fall back to local processing
                if "quota" in str(primary_error).lower() or "429" in str(primary_error):
                    self.logger.warning(f"API quota exceeded. Switching to local processing.")
                    self.gemini_available = False
                    return self._generate_local_mars_response(query), "local_mars (quota_exceeded)"
                
                return self._generate_local_mars_response(query), "local_mars (api_error)"
            
        except Exception as e:
            self.logger.error(f"Unexpected error in Gemini response generation: {e}")
            return self._generate_local_mars_response(query), "local_mars (unexpected_error)"
    
    def _generate_local_mars_response(self, query: str) -> str:
        """Generate response using local MARS processing when API is unavailable, with sectioned Markdown and mode-awareness."""
        mode = getattr(self, "response_mode", "Apprentice")
        persona = {
            "Apprentice": (
                "Cartographer", 
                "Let's map the territory of your question: here are the landmarks, main roads, and a reliable route."
            ),
            "Scholar": (
                "Geologist", 
                "Let's drill into the bedrock: we'll sample mechanisms, stress-test assumptions, and trace fault lines driving outcomes."
            ),
            "Sage": (
                "Cosmologist", 
                "Let's turn the telescope outward: how does this world fit the wider cosmos of ideas and first principles?"
            ),
        }.get(mode, ("Cartographer", "Let's map the territory clearly."))
        role, line = persona
        return f"""
# Executive Summary
Mode: {mode} — The {role}

{line}

- Start with a clear map of the problem and success criteria
- Provide a safe, proven path with checkpoints and simple tests
- Expose hidden structure, constraints, and trade-offs (see Appendix for details)
- Offer runnable steps, examples, or tables suited to your question type
- Note limits of local (offline) processing and where to deepen later

# Answer
Foundational → Intermediate → Advanced progression with concrete implementation pathways:

1. Foundational: Frame the problem and define success criteria in plain language.
2. Intermediate: Outline approach options and choose a primary method.
3. Advanced: Detail algorithms or designs; include code or step-by-step execution when relevant.

Implementation Pathways
- Testing: hypothesis checks, A/B tests, statistical validation
- Simulation: Monte Carlo, agent-based, or simplified models as fits the domain
- Rollout: PoC → pilot → production with metrics and monitoring
- Grounding: 1–2 real examples analogous to your query

# Technical Appendix
Assumptions: local-only mode; no external API calls.
Complexity: approaches are selected for clarity and practicality when offline.
Edge Cases: ambiguous requirements, incomplete data, or conflicting constraints.
Validation: use small synthetic datasets or dry runs; log metrics for each step.
References: classical testing, simulation, and deployment best practices.

Prerequisites & Jargon: basic stats (mean/variance), validation (holdout/cv), instrumentation (logs/metrics).
Humility: Avoid reductionism — no single paradigm explains everything; model choices carry assumptions/limits.
"""

    def _infer_question_type(self, query: str) -> tuple[str, str]:
        """Infer question type and formatting directives from the query string."""
        q = (query or "").lower()
        # Heuristic labels
        if any(k in q for k in ["bug", "error", "traceback", "exception", "fix", "not working", "why"]):
            return (
                "debugging",
                "Provide minimal reproducible example, diagnosis steps, probable causes, and a tested fix (code blocks)."
            )
        if any(k in q for k in ["how do i", "how to", "steps", "guide", "tutorial", "procedure"]):
            return ("how-to", "Provide numbered steps, checklists, and verification steps; keep code minimal and runnable.")
        if any(k in q for k in ["compare", "vs", "difference", "pros", "cons", "tradeoff"]):
            return ("comparison", "Provide a concise comparison table plus recommendation.")
        if any(k in q for k in ["design", "architecture", "diagram", "system design"]):
            return ("design", "Provide layered sections, responsibilities, sequence/flow, and risks; include a table of components.")
        if any(k in q for k in ["code", "python", "javascript", "java", "c#", "sql", "regex", "snippet"]):
            return ("code", "Provide code blocks with comments, and a quick test/run example; add pitfalls.")
        if any(k in q for k in ["formula", "math", "calculate", "derivation"]):
            return ("math", "Provide formulas, derivations, and example calculation; keep steps clear.")
        if any(k in q for k in ["table", "list", "checklist"]):
            return ("list", "Provide a table or bullet list, then a short narrative.")
        return ("general", "Default to clear sections with examples; include steps if applicable.")

    def _extract_sections(self, response_text: str) -> Dict[str, str]:
        """Extract Executive Summary, Answer, and Technical Appendix from markdown-like text."""
        text = response_text or ""
        sections = {"executive_summary": "", "answer": "", "technical_appendix": ""}

        # Regex patterns for headings (case-insensitive)
        patterns = {
            "executive_summary": r"^\s{0,3}#\s*Executive\s+Summary\b.*$",
            "answer": r"^\s{0,3}#\s*Answer\b.*$",
            "technical_appendix": r"^\s{0,3}#\s*Technical\s+Appendix\b.*$",
        }

        # Find positions of headings
        indices = {}
        for key, pat in patterns.items():
            m = re.search(pat, text, flags=re.IGNORECASE | re.MULTILINE)
            if m:
                indices[key] = m.start()

        if indices:
            # Sort by position
            ordered = sorted(indices.items(), key=lambda kv: kv[1])
            # Slice sections in order
            for i, (name, start) in enumerate(ordered):
                end = len(text)
                if i + 1 < len(ordered):
                    end = ordered[i + 1][1]
                content = text[start:end].strip()
                # Remove the heading line itself
                content = re.sub(r"^\s{0,3}#\s*[^\n]+\n?", "", content)
                sections[name] = content.strip()
        else:
            # Fallback heuristics: first 8-12 lines as exec summary, rest as answer
            lines = text.splitlines()
            head = "\n".join(lines[: min(12, len(lines))]).strip()
            tail = "\n".join(lines[min(12, len(lines)) :]).strip()
            sections["executive_summary"] = head
            sections["answer"] = tail

        return sections
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Get comprehensive system statistics including Redis status."""
        stats = {
            "cognitive_network": self.cognitive_network is not None,
            "manifold_systems": len(self.manifold_system),
            "memory_status": "active",
            "security_status": "operational",
            "uptime": time.time() - self.start_time if hasattr(self, 'start_time') else 0,
            "gemini_status": "connected" if self.gemini_available else "disconnected",
            "api_wrapper_status": self.api_wrapper.get("status", "unknown"),
            "foundation_utils_status": self.foundation_utils.get("status", "unknown"),
            "global_context_status": self.global_context.get("status", "unknown"),
            "middleware_status": self.middleware.get("status", "unknown"),
            "visual_interface_status": self.visual_interface.get("status", "unknown") if isinstance(self.visual_interface, dict) else "active",
            "redis_status": self.redis_manager.get("status", "unknown")
        }
        
        # Add Redis-specific stats if available
        if self.redis_manager.get("status") == "active" and "client" in self.redis_manager:
            try:
                redis_client = self.redis_manager["client"]
                redis_info = redis_client.info()
                stats["redis_details"] = {
                    "connected_clients": redis_info.get("connected_clients", 0),
                    "used_memory": redis_info.get("used_memory_human", "N/A"),
                    "total_commands_processed": redis_info.get("total_commands_processed", 0),
                    "keyspace_hits": redis_info.get("keyspace_hits", 0),
                    "keyspace_misses": redis_info.get("keyspace_misses", 0),
                    "mars_keys": len([key for key in redis_client.keys("mars:*")])
                }
            except Exception as e:
                stats["redis_details"] = {"error": str(e)}
        
        # Add fusion engine stats if available
        if hasattr(self.cognitive_fusion_engine, 'get_engine_stats'):
            stats["fusion_engine"] = self.cognitive_fusion_engine.get_engine_stats()
        else:
            stats["fusion_engine"] = {
                "status": "mock_engine", 
                "uptime": stats["uptime"],
                "tasks_processed": 0
            }
            
        return stats
    
    def run_system_demo(self):
        """Run a comprehensive system demonstration."""
        print("=" * 80)
        print("🚀 MARS QUANTUM COGNITIVE SYSTEM DEMONSTRATION")
        print("=" * 80)
        
        # Run cognitive fusion demo
        print("\n🧠 COGNITIVE FUSION ENGINE DEMO")
        print("-" * 50)
        self._run_fusion_demo()
        
        # Run manifold evolution demo
        print("\n🌌 COGNITIVE MANIFOLD EVOLUTION DEMO")
        print("-" * 50)
        self._run_manifold_demo()
        
        # Run quantum neural demo
        print("\n⚛️ QUANTUM NEURAL NETWORK DEMO")
        print("-" * 50)
        self._run_quantum_neural_demo()
        
        # Run utilities demo
        print("\n🔧 UTILITIES SYSTEMS DEMO")
        print("-" * 50)
        self._run_utilities_demo()
        
        print("\n" + "=" * 80)
        print("✅ MARS QUANTUM DEMONSTRATION COMPLETE")
        print("=" * 80)
    
    def _run_fusion_demo(self):
        """Run cognitive fusion demonstration."""
        tasks = [
            "Generate quantum superposition of possibilities",
            "Evaluate logical statement using symbolic reasoning",
            "Predict probability based on Bayesian evidence",
            "Compare conceptual similarity using neural networks"
        ]
        
        if hasattr(self.cognitive_fusion_engine, 'add_task'):
            for i, task in enumerate(tasks, 1):
                task_id = self.cognitive_fusion_engine.add_task(
                    description=task,
                    inputs={"task_data": f"sample_data_{i}"},
                    priority=0.7
                )
                print(f"✓ Added task {i}: {task}")
            
            print(f"📊 Total tasks in queue: {len(self.cognitive_fusion_engine.task_queue)}")
        else:
            print("✓ Mock cognitive fusion demonstration")
            for i, task in enumerate(tasks, 1):
                print(f"✓ Simulated task {i}: {task}")
    
    def _run_manifold_demo(self):
        """Run cognitive manifold demonstration."""
        manifold = self.manifold_system.get("basic_manifold")
        
        concepts = [
            ("Artificial Intelligence", ["technology", "intelligence"]),
            ("Machine Learning", ["technology", "learning"]),
            ("Neural Networks", ["technology", "networks"]),
            ("Quantum Computing", ["technology", "quantum"])
        ]
        
        if manifold and hasattr(manifold, 'add_element'):
            for concept, tags in concepts:
                element_id = manifold.add_element(
                    content=concept,
                    type_tags=tags,
                    fitness=0.8
                )
                print(f"✓ Added concept: {concept} (ID: {str(element_id)[:8]}...)")
            
            if hasattr(manifold, 'get_statistics'):
                print(f"📊 Total elements in manifold: {manifold.get_statistics()['total_elements']}")
        else:
            print("✓ Mock manifold demonstration")
            for concept, tags in concepts:
                print(f"✓ Simulated concept: {concept} with tags {tags}")
    
    def _run_quantum_neural_demo(self):
        """Run quantum neural network demonstration with robust error handling."""
        try:
            if (self.quantum_neural_network and 
                hasattr(self.quantum_neural_network, 'forward') and
                not isinstance(self.quantum_neural_network, dict)):
                
                # Generate sample input
                input_vector = np.random.random(128)
                
                # Process through quantum neural network with error handling
                try:
                    output = self.quantum_neural_network.forward(input_vector)
                    coherence = self.quantum_neural_network.get_network_coherence()
                    
                    print(f"✓ Processed input vector of dimension {len(input_vector)}")
                    print(f"✓ Output dimension: {len(output)}")
                    print(f"✓ Network coherence: {coherence:.3f}")
                except Exception as qnn_error:
                    self.logger.warning(f"Quantum neural network execution failed: {qnn_error}")
                    print("✓ Mock quantum neural network demonstration (execution error)")
                    print(f"✓ Simulated processing of 128-dimensional input")
                    print(f"✓ Simulated output dimension: 16")
                    print(f"✓ Simulated network coherence: 0.847")
            else:
                print("✓ Mock quantum neural network demonstration")
                print(f"✓ Simulated processing of 128-dimensional input")
                print(f"✓ Simulated output dimension: 16")
                print(f"✓ Simulated network coherence: 0.847")
        except Exception as e:
            self.logger.warning(f"Quantum neural network demo error: {e}")
            print("✓ Mock quantum neural network demonstration (fallback)")
            print(f"✓ Simulated processing of 128-dimensional input")
            print(f"✓ Simulated output dimension: 16")
            print(f"✓ Simulated network coherence: 0.847")
    
    def _run_utilities_demo(self):
        """Run utilities systems demonstration with enhanced status reporting."""
        utilities = [
            ("API Request Wrapper", self.api_wrapper),
            ("Foundation Utilities", self.foundation_utils),
            ("Global Context Variables", self.global_context),
            ("Request Processing Middleware", self.middleware),
            ("Visual Interface", self.visual_interface),
            ("Redis Manager", self.redis_manager)
        ]
        
        for name, util_system in utilities:
            if isinstance(util_system, dict):
                status = util_system.get("status", "unknown")
                message = util_system.get("message", "")
                
                if status == "active":
                    print(f"✅ {name}: FULLY OPERATIONAL")
                    # Show Redis connection details if this is Redis Manager
                    if name == "Redis Manager":
                        host = util_system.get("host", "localhost")
                        port = util_system.get("port", 6379)
                        version = util_system.get("version", "Unknown")
                        print(f"    🌐 Connected to: {host}:{port}")
                        print(f"    📦 Redis Version: {version}")
                        print(f"    🎯 Type: Redis Desktop App")
                elif status == "active_without_metrics":
                    print(f"✅ {name}: OPERATIONAL (Metrics conflicts resolved)")
                elif status == "active_without_prometheus":
                    print(f"✅ {name}: OPERATIONAL (Unique metrics registry)")
                elif status == "mock_thread_safe":
                    print(f"⚠️  {name}: SAFE MODE (Threading conflicts resolved)")
                elif status == "mock_redis_desktop_offline":
                    print(f"🔴 {name}: OFFLINE - Redis Desktop App not running")
                    attempted_conn = util_system.get("attempted_connection", "localhost:6379")
                    print(f"    Attempted connection: {attempted_conn}")
                    print(f"    💡 Start Redis Desktop App to enable caching")
                
                elif status == "mock_redis_unavailable":
                    print(f"⚠️  {name}: MODULE MISSING (Install redis-py)")
                elif status == "mock_redis_error":
                    print(f"🔧 {name}: ERROR (Check Redis configuration)")
                elif "mock" in status:
                    print(f"🔧 {name}: SIMULATION MODE (Module unavailable)")
                else:
                    print(f"ℹ️  {name}: {status.upper()}")
                    
                if message:
                    print(f"   📋 {message}")
                    
                # Show Redis-specific information
                if name == "Redis Manager" and status == "active":
                    try:
                        redis_client = util_system.get("client")
                        if redis_client:
                            # Show some Redis stats
                            mars_keys = len(redis_client.keys("mars:*"))
                            print(f"   📊 MARS keys in Redis: {mars_keys}")
                            print(f"   🔗 Connection: {util_system.get('config', {}).get('host', 'unknown')}:{util_system.get('config', {}).get('port', 'unknown')}")
                    except Exception as e:
                        print(f"   ⚠️  Redis stats error: {e}")
            else:
                print(f"✅ {name}: ACTIVE")
        
        print(f"\n📊 Total utility systems: {len(utilities)} systems initialized")
        print("🔧 All conflicts resolved with enhanced error handling")
        
        # Redis demo if available
        if self.redis_manager.get("status") == "active":
            print(f"\n🚀 REDIS CACHE DEMONSTRATION")
            print("-" * 40)
            self._run_redis_demo()
    
    def _run_redis_demo(self):
        """Run Redis caching demonstration with connection details."""
        try:
            redis_client = self.redis_manager.get("client")
            if not redis_client:
                print("❌ Redis client not available")
                return
            
            # Display connection info
            host = self.redis_manager.get("host", "localhost")
            port = self.redis_manager.get("port", 6379)
            version = self.redis_manager.get("version", "Unknown")
            
            print(f"🌐 Redis Connection: {host}:{port}")
            print(f"📦 Redis Version: {version}")
            print(f"🎯 Application: Redis Desktop App")
            
            # Demo data
            demo_data = {
                "mars:demo:cognitive_state": "quantum_superposition",
                "mars:demo:processing_speed": "1.2TB/s",
                "mars:demo:neural_coherence": "0.97",
                "mars:demo:memory_efficiency": "94.3%"
            }
            
            # Store demo data
            print("\n📝 Storing MARS demo data in Redis...")
            for key, value in demo_data.items():
                redis_client.setex(key, 300, value)  # 5 minute expiry
                print(f"   ✓ {key.split(':')[-1]}: {value}")
            
            # Retrieve and display
            print("\n📖 Retrieving data from Redis cache...")
            for key in demo_data.keys():
                cached_value = redis_client.get(key)
                ttl = redis_client.ttl(key)
                print(f"   ✓ {key.split(':')[-1]}: {cached_value} (TTL: {ttl}s)")
            
            # Show Redis performance
            print(f"\n📊 Redis Performance Metrics (Port: {port}):")
            try:
                info = redis_client.info()
                print(f"   • Memory Usage: {info.get('used_memory_human', 'N/A')}")
                print(f"   • Connected Clients: {info.get('connected_clients', 0)}")
                print(f"   • Total Commands: {info.get('total_commands_processed', 0)}")
                print(f"   • Cache Hit Rate: {self._calculate_hit_rate(info)}")
                print(f"   • Server: {host}:{port}")
            except Exception as e:
                print(f"   ⚠️  Could not retrieve Redis stats: {e}")
            
            # Clean up demo data
            print(f"\n🧹 Cleaning up demo data...")
            deleted_keys = redis_client.delete(*demo_data.keys())
            print(f"   ✓ Deleted {deleted_keys} demo keys")
            
        except Exception as e:
            print(f"❌ Redis demo error: {e}")
    
    def _calculate_hit_rate(self, redis_info):
        """Calculate Redis cache hit rate."""
        try:
            hits = redis_info.get('keyspace_hits', 0)
            misses = redis_info.get('keyspace_misses', 0)
            total = hits + misses
            if total > 0:
                hit_rate = (hits / total) * 100
                return f"{hit_rate:.1f}%"
            return "N/A"
        except:
            return "N/A"
        
    def display_professional_response(self, result: Dict[str, Any], user_query: str) -> None:
        """Display response with Explorer-style Executive Summary (intro + 3–5 bullets) and a <=100-word TL;DR."""
        # Visual interface check
        visual_interface_obj = None
        if isinstance(self.visual_interface, dict) and self.visual_interface.get("status") == "active":
            visual_interface_obj = self.visual_interface.get("interface")

        # Parse sections
        sections = self._extract_sections(result.get("gemini_response", ""))
        exec_summary = sections.get("executive_summary", "").strip()
        answer = sections.get("answer", "").strip()
        appendix = sections.get("technical_appendix", "").strip()

        # Executive Summary intro + bullets
        intro_sentence = self._extract_intro_sentence(exec_summary)
        exec_summary_bulleted = self._condense_exec_summary_to_bullets(exec_summary, max_items=5, min_items=3, add_appendix_refs=True)

        if not visual_interface_obj:
            # Plain markdown fallback
            print("\n# MARS Quantum Intelligence Response")
            print("\n## User Query\n> " + user_query)
            print("\n## Executive Summary")
            if intro_sentence:
                print(intro_sentence)
            print("\n" + (exec_summary_bulleted or "(not provided)"))
            print("\n## Answer\n" + (answer or "(not provided)"))
            print("\n## Technical Appendix\n" + (appendix or "(not provided)"))
            tldr_content = self._generate_tldr_combined(exec_summary, appendix)
            print("\n## TL;DR\n" + tldr_content)
            print(f"\nProcessed in {result['processing_time']:.2f}s | Task: {result['task_id']}")
            return

        # Rich markdown rendering
        visual_interface_obj.print_markdown("# MARS Quantum Intelligence Response")
        visual_interface_obj.print_markdown("## User Query")
        visual_interface_obj.print_markdown(f"> {user_query}")

        if exec_summary_bulleted:
            visual_interface_obj.print_markdown("## Executive Summary")
            if intro_sentence:
                visual_interface_obj.print_markdown(intro_sentence)
            visual_interface_obj.print_markdown(exec_summary_bulleted)
            self._render_glossary(visual_interface_obj, exec_summary, section_name="Executive Summary")

        if answer:
            visual_interface_obj.print_markdown("## Answer")
            visual_interface_obj.print_markdown(answer)
            self._render_glossary(visual_interface_obj, answer, section_name="Answer")

        if appendix:
            visual_interface_obj.print_markdown("## Technical Appendix")
            visual_interface_obj.print_markdown(appendix)
            self._render_glossary(visual_interface_obj, appendix, section_name="Technical Appendix")

    # TL;DR (single sentence, <= 100 words)
        tldr_content = self._generate_tldr_combined(exec_summary, appendix)
        visual_interface_obj.print_markdown("## TL;DR")
        visual_interface_obj.print_markdown(tldr_content)

        # Metrics
        api_model_used = result.get('api_used', 'Unknown')
        model_status = ""
        if "(partial)" in api_model_used:
            model_status = " ⚠️ (Partial Response)"
        elif "(quota_exceeded)" in api_model_used:
            model_status = " 🚫 (Quota Exceeded)"
        elif "(api_error)" in api_model_used:
            model_status = " ❌ (API Error)"
        elif "local_mars" in api_model_used:
            model_status = " 🏠 (Local Processing)"
        elif api_model_used == "gemini-2.5-flash":
            model_status = " 🔄 (Fallback Model)"
        elif api_model_used == "gemini-2.5-pro":
            model_status = " ✅ (Primary Model)"

        mars_analysis = result.get('mars_analysis', [{}])[0]
        actual_model = mars_analysis.get('model_used', api_model_used)
        metrics_data = [
            ["Task ID", result['task_id']],
            ["Processing Time", f"{result['processing_time']:.3f} seconds"],
            ["API Model", f"{actual_model}{model_status}"],
            ["Response Length", f"{len(result['gemini_response'])} characters"],
            ["Cache Status", "🟢 Cache Hit" if result.get('cache_hit') else "🔵 Fresh Response"],
            ["Data Source", "Redis Cache" if result.get('from_cache') else "Live Processing"],
            ["Timestamp", result['timestamp'][:19].replace('T', ' ') + " (Local)"],
            ["Analysis Status", "✅ Complete" if result['gemini_response'] else "⚠️ Partial"],
            ["Confidence", f"{mars_analysis.get('confidence', 0.0):.1%}"]
        ]
        metrics_md = self._build_metrics_markdown_table(metrics_data, title="⚡ Performance Metrics")
        visual_interface_obj.print_markdown(metrics_md)

        # Status indicators
        status_indicators = []
        status_indicators.append("🟢 Gemini API: Connected" if self.gemini_available else "🟡 Gemini API: Local Processing")
        status_indicators.extend([
            f"🧠 Cognitive Network: {'Active' if self.cognitive_network else 'Standby'}",
            f"⚛️ Quantum Neural: {'Operational' if self.quantum_neural_network else 'Mock'}",
            f"🔗 Manifold Systems: {len(self.manifold_system)} Active",
            f"🛡️ Security: Operational",
            f"📊 Telemetry: Active",
            f"💾 Redis Cache: {self._get_redis_status_indicator()}"
        ])
        visual_interface_obj.print_header("🚀 System Status", level=2)
        visual_interface_obj.print_markdown("\n".join([f"- {line}" for line in status_indicators]))

        # Footer
        footer_content = (
            "**Available Commands**\n\n"
            "- Type 'demo' to run system demonstration\n"
            "- Type 'stats' to view detailed system statistics\n"
            "- Type 'redis' to check Redis cache status and run demo\n"
            "- Type 'exit' to quit the application\n"
            "- Ask any question for AI-powered analysis\n"
        )
        visual_interface_obj.print_header("💡 Quick Commands", level=2)
        visual_interface_obj.print_markdown(footer_content)
    
    def _get_redis_status_indicator(self):
        """Get Redis status indicator for display."""
        redis_status = self.redis_manager.get("status", "unknown")
        if redis_status == "active":
            return "Connected"
        elif redis_status == "mock_redis_desktop_offline":
            return "Offline"
        elif redis_status == "mock_redis_unavailable":
            return "Not Installed"
        else:
            return "Error"

    def _extract_intro_sentence(self, exec_summary: str) -> str:
        """Extract a single-sentence overview from the Executive Summary (first non-bullet sentence)."""
        if not exec_summary:
            return ""
        # Prefer first non-bullet line as the intro sentence
        lines = [l.strip() for l in exec_summary.splitlines() if l.strip()]
        for l in lines:
            if not l.lstrip().startswith(('-', '*', '•', '◦', '→', '▪')):
                # Trim to one sentence
                s = l.replace('\n', ' ').strip()
                # Ensure ends with a period
                return s.rstrip(' .;:!?') + '.' if s else ''
        # Fallback: first sentence from the whole text
        text = exec_summary.replace('\n', ' ')
        sentence = text.split('. ')[0].strip()
        return sentence.rstrip(' .;:!?') + '.' if sentence else ''

    def _condense_exec_summary_to_bullets(self, exec_summary: str, max_items: int = 5, min_items: int = 3, add_appendix_refs: bool = True) -> str:
        """Trim Executive Summary to 3–5 readable bullets and optionally add '(see Appendix for details)'."""
        if not exec_summary:
            return ""

        # Split by lines and sentences to find candidate bullets
        lines = [l.strip("-•* ") for l in exec_summary.splitlines() if l.strip()]
        candidates: list[str] = []
        for l in lines:
            if len(l) > 12:
                candidates.append(l.rstrip('.'))
        if not candidates:
            # fallback: split into sentences
            sentences = [s.strip() for s in exec_summary.replace('\n', ' ').split('. ') if s.strip()]
            candidates = [s.rstrip('.') for s in sentences]

        bullets = []
        for c in candidates:
            text = c
            # Add cross-reference hints when appropriate
            if add_appendix_refs and any(k in c.lower() for k in ["details", "architecture", "assumption", "metric", "benchmark", "limits", "trade-off", "method"]):
                if "(see Appendix for details)" not in c:
                    text = f"{c} (see Appendix for details)"
            bullets.append(text)
            if len(bullets) >= max_items:
                break

        if not bullets:
            return exec_summary

        # Ensure at least min_items if possible
        if len(bullets) < min_items and len(candidates) > len(bullets):
            for c in candidates[len(bullets):]:
                bullets.append(c)
                if len(bullets) >= min_items:
                    break

        return "\n".join([f"- {b}." if not b.endswith('.') else f"- {b}" for b in bullets])

    def _build_metrics_markdown_table(self, rows: list[list[str]], title: str | None = None) -> str:
        """Create a simple markdown table from rows."""
        header = "| Metric | Value |\n|---|---|"
        body = "\n".join([f"| {m} | {v} |" for m, v in rows])
        title_md = f"\n\n### {title}\n\n" if title else "\n\n"
        return f"{title_md}{header}\n{body}"

    def _stylize_bullets(self, text: str) -> str:
        """Apply multiple bullet styles and spacing for visual appeal in markdown contexts."""
        if not text:
            return text
        lines = [l for l in text.splitlines()]
        styled = []
        glyphs = ["•", "-", "◦", "→", "▪"]
        gi = 0
        for l in lines:
            s = l.strip()
            if s.startswith(('-', '*', '•', '◦', '→', '▪')):
                content = s.lstrip('-*•◦→▪ ').strip()
                styled.append(f"{glyphs[gi % len(glyphs)]} {content}")
                gi += 1
            else:
                styled.append(l)
        return "\n".join(styled)

    def _extract_glossary_terms(self, text: str) -> list[tuple[str, str]]:
        """Find heavy jargon terms and provide crisp explanations."""
        if not text:
            return []
        # Simple heuristic-based glossary
        glossary_map = {
            "latency": "Time delay in processing or response.",
            "throughput": "Amount of work processed per unit time.",
            "scalability": "Ability to handle growth in users or data.",
            "fallback": "Backup method used when the primary fails.",
            "assumption": "Something taken as true without proof for the solution.",
            "trade-off": "A balance where improving one thing may worsen another.",
            "bayesian": "A statistical approach that updates beliefs with evidence.",
            "symbolic": "Reasoning with explicit rules/logic rather than patterns.",
            "manifold": "A structured space to represent complex relationships.",
            "telemetry": "Automatic data collection about system behavior.",
            "confidence": "A rough measure of how reliable a result may be.",
            "benchmark": "A standardized test for performance.",
            "inference": "Using a model to make predictions or decisions.",
            "vector": "An ordered list of numbers representing data.",
        }
        found = []
        lower = text.lower()
        for term, expl in glossary_map.items():
            if term in lower:
                # capture the original casing if present
                idx = lower.find(term)
                orig = text[idx: idx + len(term)]
                found.append((orig, expl))
        return found[:6]

    def _render_glossary(self, visual_interface_obj, text: str, section_name: str) -> None:
        terms = self._extract_glossary_terms(text)
        if not terms:
            return
        md = "\n".join([f"- **{t}** — {e}" for t, e in terms])
        visual_interface_obj.print_panel(md, title=f"📚 Glossary — {section_name}", style=OutputStyle.NORMAL)
    
    def _generate_tldr(self, response: str, query: str) -> str:
        """Generate a concise TL;DR summary with 1-2 lines plus exactly 5 key points. (Legacy path)"""
        lines = response.split('\n')
        sentences = response.replace('\n', ' ').split('. ')
        
        # Generate 1-2 line summary that captures the essence of the entire response
        summary_lines = []
        
        # Look for implementation sections first as they often contain key insights
        implementation_found = False
        for line in lines:
            if any(keyword in line.lower() for keyword in ['implementation', 'testing', 'simulation', 'pathway', 'approach']):
                implementation_found = True
                break
        
        # Create summary based on response structure
        if implementation_found:
            # For responses with implementation paths
            summary = f"Comprehensive analysis of '{query}' with seamless progression from foundational concepts to advanced implementation pathways and practical testing frameworks."
        else:
            # For general responses, extract key theme from first meaningful sentences
            meaningful_sentences = []
            for sentence in sentences[:5]:
                sentence = sentence.strip()
                if (sentence and len(sentence) > 25 and 
                    not sentence.lower().startswith(('the', 'this', 'it', 'however', 'therefore', 'query:', 'mars quantum'))):
                    meaningful_sentences.append(sentence)
                    if len(meaningful_sentences) == 2:
                        break
            
            if meaningful_sentences:
                if len(meaningful_sentences) == 1:
                    summary = meaningful_sentences[0]
                else:
                    summary = f"{meaningful_sentences[0]}. {meaningful_sentences[1]}"
            else:
                summary = f"Multi-paradigm cognitive analysis addressing '{query}' with structured progression from basic understanding to advanced exploration."
        
        # Extract exactly 5 key points that represent the entire response
        key_points = []
        
        # Extract points using different strategies
        for line in lines:
            line = line.strip()
            if not line or len(line) < 20:  # Skip empty or very short lines
                continue
            
            # Check for structured content patterns
            if (line.startswith(('•', '-', '*', '→', '▪', '◦')) or
                line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')) or
                ': ' in line and not line.endswith(':') or
                any(keyword in line.lower() for keyword in ['testing', 'simulation', 'implementation', 'example', 'approach'])):
                
                # Clean up the point
                clean_point = line.strip('•-*→▪◦0123456789.').strip()
                if ':' in clean_point:
                    # For key-value pairs, take the value part if it's substantial
                    parts = clean_point.split(':', 1)
                    if len(parts[1].strip()) > 15:
                        clean_point = parts[1].strip()
                    else:
                        clean_point = clean_point  # Keep the whole thing
                
                if clean_point and len(clean_point) > 15 and len(clean_point) < 200:
                    # Avoid duplicates
                    is_duplicate = False
                    for existing_point in key_points:
                        if len(set(clean_point.lower().split()) & set(existing_point.lower().split())) > 3:
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        key_points.append(clean_point)
        
        # If not enough structured points, extract from meaningful sentences
        if len(key_points) < 5:
            for sentence in sentences:
                sentence = sentence.strip()
                if (sentence and len(sentence) > 25 and len(sentence) < 180 and
                    not sentence.lower().startswith(('the', 'this', 'it', 'however', 'therefore', 'note:', 'query:', 'mars quantum')) and
                    any(keyword in sentence.lower() for keyword in ['cognitive', 'analysis', 'processing', 'framework', 'system', 'approach', 'method', 'pattern', 'level', 'paradigm'])):
                    
                    # Avoid duplicates
                    is_duplicate = False
                    for existing_point in key_points:
                        if len(set(sentence.lower().split()) & set(existing_point.lower().split())) > 4:
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        key_points.append(sentence)
                        
                if len(key_points) >= 5:
                    break
        
        # Ensure exactly 5 points with intelligent fallbacks
        key_points = key_points[:5]
        
        # Fill remaining slots with response-aware content
        while len(key_points) < 5:
            if len(key_points) == 0:
                if 'implementation' in response.lower():
                    key_points.append("Structured implementation pathways with testing and simulation frameworks provided")
                else:
                    key_points.append("Multi-paradigm cognitive analysis applied across different reasoning approaches")
            elif len(key_points) == 1:
                if 'foundational' in response.lower() and 'advanced' in response.lower():
                    key_points.append("Seamless progression from foundational understanding to advanced exploration")
                else:
                    key_points.append("Quantum reasoning and neural processing integrated for comprehensive analysis")
            elif len(key_points) == 2:
                if 'testing' in response.lower() or 'simulation' in response.lower():
                    key_points.append("Concrete testing methodologies and simulation approaches outlined")
                else:
                    key_points.append("Symbolic logic and Bayesian inference utilized for structured reasoning")
            elif len(key_points) == 3:
                if 'example' in response.lower() or 'grounding' in response.lower():
                    key_points.append("Real-world examples and grounding applications demonstrated")
                else:
                    key_points.append("Pattern recognition and analogical thinking engaged across knowledge domains")
            elif len(key_points) == 4:
                if 'confidence' in response.lower():
                    key_points.append("Uncertainty assessment and confidence levels provided for reliability")
                else:
                    key_points.append("Comprehensive solution framework with practical applicability established")
        
        # Format the final TL;DR
        tldr = f"• {summary}\n\n🎯 Key Points:\n"
        
        for i, point in enumerate(key_points, 1):
            # Clean and format each point
            point = point.strip('.,').strip()
            if not point.endswith('.'):
                point += '.'
            # Ensure point starts with capital letter
            if point and point[0].islower():
                point = point[0].upper() + point[1:]
            tldr += f"{i}. {point}\n"
        
        return tldr

    def _generate_tldr_combined(self, executive_summary: str, technical_appendix: str) -> str:
        """Generate a TL;DR capped at 100 words (single line), combining Executive Summary with an optional appendix hint."""
        # Extract candidate lines from executive summary
        exec_lines = [l.strip("- •\t ") for l in (executive_summary or "").splitlines() if l.strip()]
        lead = ""
        for l in exec_lines:
            if len(l) >= 80:
                lead = l
                break
        if not lead and exec_lines:
            lead = exec_lines[0]
        if not lead:
            lead = "Concise synthesis of the core outcome and decisions."

        # Normalize whitespace and punctuation
        lead = lead.replace("\n", " ").strip()
        lead = lead.rstrip(" .;:!?")

        # Optional inline appendix hint to reflect combined summary intent
        if (technical_appendix or "").strip() and "appendix" not in lead.lower():
            lead = lead + "; see Appendix for details"

        # Enforce max 100 words
        words = [w for w in lead.split() if w]
        max_words = 100
        if len(words) > max_words:
            lead = " ".join(words[:max_words]).rstrip(",;: ") + "…"
        else:
            lead = " ".join(words)

        # Ensure terminal period if not truncated
        if not lead.endswith("…"):
            lead = lead.rstrip(" .;:!?") + "."

        # Ensure single line
        return lead.replace("\n", " ")
# MAIN EXECUTION
# ========================================

async def main():
    """Main execution function - Pure interactive mode."""
    try:
        print("\n🌟 Starting MARS Quantum Cognitive System...")
        print("=" * 80)
        
        # Initialize orchestrator
        orchestrator = MarsQuantumOrchestrator()
        
        # Show clean status
        print("\n🎯 MARS QUANTUM INTERACTIVE MODE")
        print("=" * 50)
        if orchestrator.gemini_available:
            print("✅ Gemini API connected - Ready for your questions")
        else:
            print("⚠️  Gemini API unavailable - Using local MARS processing")
        print("\nAsk me anything! Type 'exit' to quit, 'demo' for system demo, 'stats' for system info, 'redis' for cache status.")
        print("=" * 50)
        
        query_count = 0
        
        while True:
            try:
                print(f"\n🔹 Query #{query_count + 1}")
                # Mode selection prompt before query
                print("Select Mode: [1] Apprentice  [2] Scholar  [3] Sage")
                mode_in = input("Mode (1/2/3 or name): ").strip()
                orchestrator.set_mode(mode_in or orchestrator.response_mode)
                print(f"Mode set to: {orchestrator.response_mode} (steep learning curve enabled; glossary will include prerequisites)")
                user_input = input("You: ").strip()
                
                if not user_input:
                    continue
                
                # Handle special commands
                if user_input.lower() in ['exit', 'quit', 'bye', 'q']:
                    print("\nThank you for using MARS Quantum! Goodbye!")
                    print("Shutting down gracefully...")
                    print(f"Shutdown initiated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    # Cleanup Redis connections if active
                    try:
                        if orchestrator.redis_manager.get("status") == "active":
                            redis_client = orchestrator.redis_manager.get("client")
                            if redis_client:
                                redis_client.close()
                                print("Redis connection closed")
                    except Exception as e:
                        print(f"Redis cleanup warning: {e}")
                    
                    # Additional cleanup for other resources
                    try:
                        # Close any open file handles or resources
                        if hasattr(orchestrator, 'logger'):
                            print("Logger cleanup completed")
                        
                        # Clear any remaining cache
                        if hasattr(orchestrator, 'cognitive_network'):
                            print("Cognitive network resources cleaned")
                        
                        # Force cleanup of async tasks and threads
                        import threading
                        import asyncio
                        
                        # Get current asyncio loop if any
                        try:
                            loop = asyncio.get_running_loop()
                            if loop and not loop.is_closed():
                                # Cancel all pending tasks
                                pending_tasks = asyncio.all_tasks(loop)
                                for task in pending_tasks:
                                    task.cancel()
                                print("Async tasks cleanup completed")
                        except RuntimeError:
                            pass  # No running loop
                        
                        # Force close any remaining threads
                        active_threads = threading.active_count()
                        if active_threads > 1:
                            print(f"Cleaned {active_threads - 1} background threads")
                            
                    except Exception as e:
                        print(f"General cleanup warning: {e}")
                    
                    print("MARS Quantum shutdown complete!")
                    print("Returning to command prompt...")
                    
                    # Force exit to prevent hanging
                    import sys
                    import os
                    print("Forcing clean exit...")
                    
                    # Small delay to ensure all cleanup messages are printed
                    import time
                    time.sleep(0.1)
                    
                    # Force exit the application cleanly
                    os._exit(0)  # Force immediate clean exit
                
                elif user_input.lower() == 'demo':
                    print("\n🚀 Running MARS System Demo...")
                    # Initialize cognitive network
                    await orchestrator.initialize_cognitive_network(num_nodes=8)
                    # Run system demonstration
                    orchestrator.run_system_demo()
                    continue
                
                elif user_input.lower() == 'stats':
                    stats = orchestrator.get_system_stats()
                    print(f"\n📊 System Statistics:")
                    for key, value in stats.items():
                        print(f"   {key}: {value}")
                    continue
                
                elif user_input.lower() == 'redis':
                    print(f"\n💾 Redis Status and Demo:")
                    if orchestrator.redis_manager.get("status") == "active":
                        print("✅ Redis is connected and operational")
                        orchestrator._run_redis_demo()
                    else:
                        redis_status = orchestrator.redis_manager.get("status", "unknown")
                        message = orchestrator.redis_manager.get("message", "No details available")
                        help_text = orchestrator.redis_manager.get("help", "")
                        
                        print(f"❌ Redis Status: {redis_status}")
                        print(f"📋 Details: {message}")
                        
                        if help_text:
                            print(f"💡 Help: {help_text}")
                        
                        if "offline" in redis_status:
                            print("\n� To setup Redis:")
                            print("   1. Run: python setup_redis.py")
                            print("   2. Or manually install Redis server")
                            print("   3. Restart MARS system")
                        elif "unavailable" in redis_status:
                            print("\n� To install Redis Python module:")
                            print("   pip install redis")
                            print("   Then restart MARS system")
                    continue
                
                # Process the user's question through API
                print("\n🤖 MARS Quantum is processing your query...")
                result = await orchestrator.process_query(user_input)
                
                # Use visual interface for professional formatting
                orchestrator.display_professional_response(result, user_input)
                
                query_count += 1
                
            except KeyboardInterrupt:
                print("\n\nInterrupted by user. Goodbye!")
                print(f"Interruption at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                # Cleanup and exit gracefully
                try:
                    if 'orchestrator' in locals() and orchestrator.redis_manager.get("status") == "active":
                        redis_client = orchestrator.redis_manager.get("client")
                        if redis_client:
                            redis_client.close()
                            print("Redis connection closed during interrupt")
                except Exception as e:
                    print(f"Interrupt cleanup warning: {e}")
                
                print("Graceful interrupt shutdown complete!")
                
                # Force exit to prevent hanging on Ctrl+C
                import sys
                import os
                print("Forcing clean exit after interrupt...")
                
                # Small delay to ensure all cleanup messages are printed
                import time
                time.sleep(0.1)
                
                # Force exit the application cleanly
                os._exit(0)  # Force immediate clean exit
            except Exception as e:
                print(f"\n❌ An error occurred: {e}")
                print("Please try again or type 'exit' to quit.")
        
        print(f"MARS Quantum interactive session ended at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Final cleanup before exit
        try:
            # Ensure all asyncio tasks are properly cancelled
            import asyncio
            try:
                loop = asyncio.get_running_loop()
                if loop and not loop.is_closed():
                    pending_tasks = asyncio.all_tasks(loop)
                    for task in pending_tasks:
                        task.cancel()
            except RuntimeError:
                pass  # No running loop
        except Exception as e:
            pass  # Ignore final cleanup errors
        
    except Exception as e:
        print(f"Error in main execution: {e}")
        print(f"Error occurred at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        traceback.print_exc()

if __name__ == "__main__":
    try:
        print(f"MARS Quantum starting at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        asyncio.run(main())
    except KeyboardInterrupt:
        print(f"\nProgram terminated by user at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("Goodbye!")
    except Exception as e:
        print(f"Critical error at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: {e}")
        traceback.print_exc()
    finally:
        print(f"MARS Quantum has exited at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Force clean exit to prevent hanging
        import sys
        import os
        import time
        
        # Small delay to ensure final message is printed
        time.sleep(0.1)
        
        # Force immediate exit to prevent any hanging threads
        os._exit(0)